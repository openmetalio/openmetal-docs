"use strict";(self.webpackChunkopenmetal_docs=self.webpackChunkopenmetal_docs||[]).push([[5927],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>m});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),c=p(n),h=r,m=c["".concat(l,".").concat(h)]||c[h]||d[h]||o;return n?a.createElement(m,i(i({ref:t},u),{},{components:n})):a.createElement(m,i({ref:t},u))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[c]="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},68975:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var a=n(87462),r=(n(67294),n(3905));const o={},i="How to Handle a Ceph Cluster Going Read-Only Due to Full OSDs",s={unversionedId:"tutorials/ceph-ro-guide",id:"tutorials/ceph-ro-guide",title:"How to Handle a Ceph Cluster Going Read-Only Due to Full OSDs",description:"Author: Nemanja Ilic",source:"@site/docs/tutorials/ceph-ro-guide.md",sourceDirName:"tutorials",slug:"/tutorials/ceph-ro-guide",permalink:"/docs/manuals/tutorials/ceph-ro-guide",draft:!1,editUrl:"https://github.com/openmetalio/openmetal-docs/blob/main/docs/tutorials/ceph-ro-guide.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Understanding & Breaking Ceph Parent Child Dependencies",permalink:"/docs/manuals/tutorials/ceph-dependencies"},next:{title:"Create a Site-to-Site VPN Connection with Endpoint Groups in Horizon",permalink:"/docs/manuals/tutorials/create-site-to-site-vpn-in-horizon"}},l={},p=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"What to Do When Ceph Goes RO",id:"what-to-do-when-ceph-goes-ro",level:2},{value:"1. Temporarily increase the full-ratio (safer option)",id:"1-temporarily-increase-the-full-ratio-safer-option",level:3},{value:"2. Reduce the replication factor",id:"2-reduce-the-replication-factor",level:3},{value:"Post-Recovery Cleanup Steps",id:"post-recovery-cleanup-steps",level:2},{value:"Real-World Example: A Ceph Cluster Hitting RO State",id:"real-world-example-a-ceph-cluster-hitting-ro-state",level:2},{value:"1. OSDs Approaching or Exceeding 95% Usage",id:"1-osds-approaching-or-exceeding-95-usage",level:3},{value:"2. Inspecting Cluster-Wide Usage",id:"2-inspecting-cluster-wide-usage",level:3},{value:"3. Mitigating the RO State",id:"3-mitigating-the-ro-state",level:3},{value:"How to Prevent This in the Future",id:"how-to-prevent-this-in-the-future",level:2},{value:"In Summary",id:"in-summary",level:2}],u={toc:p};function c(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"how-to-handle-a-ceph-cluster-going-read-only-due-to-full-osds"},"How to Handle a Ceph Cluster Going Read-Only Due to Full OSDs"),(0,r.kt)("p",null,"Author: Nemanja Ilic"),(0,r.kt)("p",null,"In a Ceph cluster, storage capacity reaching critical thresholds can cause the\ncluster to enter a read-only (RO) state. This typically occurs when OSDs exceed\nthe ",(0,r.kt)("inlineCode",{parentName:"p"},"full-ratio")," limit, which is set to ",(0,r.kt)("inlineCode",{parentName:"p"},"0.95")," (95%) by default. Once this\nthreshold is crossed, Ceph blocks write operations to prevent potential data\ncorruption or inconsistency. While this is a protective measure, it can bring\nproduction workloads to a halt if it happens unexpectedly."),(0,r.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://openmetal.io/docs/manuals/openmetal-central/how-to-add-ssh-keys-in-openmetal-central"},"Access to your hardware nodes."))),(0,r.kt)("p",null,"Because critical operations will take place on your ",(0,r.kt)("strong",{parentName:"p"},"physical servers"),",\nmake sure you have SSH access."),(0,r.kt)("h2",{id:"what-to-do-when-ceph-goes-ro"},"What to Do When Ceph Goes RO"),(0,r.kt)("p",null,"When this happens, you have two immediate options (run these from your\nphysical nodes):"),(0,r.kt)("h3",{id:"1-temporarily-increase-the-full-ratio-safer-option"},"1. Temporarily increase the full-ratio (safer option)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"ceph osd set-full-ratio 0.97\n")),(0,r.kt)("p",null,"This gives you a buffer to delete unnecessary data or snapshots and restore\nnormal operations."),(0,r.kt)("h3",{id:"2-reduce-the-replication-factor"},"2. Reduce the replication factor"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"ceph osd pool set images size 2\n")),(0,r.kt)("p",null,"This changes the replication factor ",(0,r.kt)("strong",{parentName:"p"},"only for the ",(0,r.kt)("inlineCode",{parentName:"strong"},"images")," pool"),", reducing it\nfrom 3 replicas to 2. It can free up space for that specific pool."),(0,r.kt)("admonition",{type:"danger"},(0,r.kt)("p",{parentName:"admonition"},(0,r.kt)("strong",{parentName:"p"},"Important:")," The\xa0number of replicas\xa0includes the object itself, ",(0,r.kt)("a",{parentName:"p",href:"https://docs.ceph.com/en/mimic/rados/operations/pools/#set-the-number-of-object-replicas"},"see the docs"),".")),(0,r.kt)("admonition",{type:"danger"},(0,r.kt)("p",{parentName:"admonition"},"Only use this in non-production or emergency scenarios, as it compromises data\ndurability.")),(0,r.kt)("p",null,"After applying either change, Ceph will lift the write block and move out of RO\nmode. In some cases, you may need to restart affected instances if operating\nsystems or applications don't handle the transition well.\n",(0,r.kt)("a",{parentName:"p",href:"https://openmetal.io/docs/manuals/tutorials/recoveraninstance"},"This guide"),"\ncan help if that happens."),(0,r.kt)("h2",{id:"post-recovery-cleanup-steps"},"Post-Recovery Cleanup Steps"),(0,r.kt)("p",null,"Once the cluster is writable again, ",(0,r.kt)("strong",{parentName:"p"},"it's crucial to act quickly"),".\nThe primary goal is to reduce OSD usage ",(0,r.kt)("strong",{parentName:"p"},"below critical thresholds"),".\nIdeally, bring OSD ",(0,r.kt)("strong",{parentName:"p"},"usage below 80%"),", or at least under 85%,\nto prevent another full state."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Focus on:")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Deleting unused volumes and snapshots"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Cleaning up abandoned or temporary data"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Working with application teams to offload unnecessary storage")))),(0,r.kt)("p",null,"Use ",(0,r.kt)("inlineCode",{parentName:"p"},"ceph osd df")," to monitor usage and ",(0,r.kt)("inlineCode",{parentName:"p"},"ceph -s")," to check for any remaining\nhealth warnings during cleanup.\n",(0,r.kt)("a",{parentName:"p",href:"https://openmetal.io/docs/manuals/operators-manual/day-2/check-ceph-status-disk-usage#check-ceph-status"},"This article"),"\nprovides additional information."),(0,r.kt)("h2",{id:"real-world-example-a-ceph-cluster-hitting-ro-state"},"Real-World Example: A Ceph Cluster Hitting RO State"),(0,r.kt)("p",null,"In this example, a Ceph cluster entered a ",(0,r.kt)("strong",{parentName:"p"},"read-only (RO)"),"\nstate due to one or more OSDs crossing the ",(0,r.kt)("inlineCode",{parentName:"p"},"full-ratio")," threshold\n(default: 0.95). The issue was identified and mitigated by temporarily\nincreasing the full-ratio, but not without immediate cleanup requirements."),(0,r.kt)("h3",{id:"1-osds-approaching-or-exceeding-95-usage"},"1. OSDs Approaching or Exceeding 95% Usage"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"[root@shiny-jacana ~]# ceph osd df\nID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP      META     AVAIL    %USE   VAR   PGS  STATUS\n 1    ssd  2.91100   1.00000  2.9 TiB  2.8 TiB  2.8 TiB   721 KiB  5.5 GiB  149 GiB  95.01  1.01  135      up\n 4    ssd  2.91100   1.00000  2.9 TiB  2.7 TiB  2.7 TiB   333 KiB  5.2 GiB  166 GiB  94.43  1.01  134      up\n 9    ssd  2.91089   1.00000  2.9 TiB  2.8 TiB  2.7 TiB   572 KiB  5.6 GiB  163 GiB  94.53  1.01  129      up\n12    ssd  2.91089   1.00000  2.9 TiB  2.8 TiB  2.8 TiB   729 KiB  5.7 GiB  157 GiB  94.72  1.01  130      up\n15    ssd  2.91089   1.00000  2.9 TiB  2.7 TiB  2.7 TiB   998 KiB  5.6 GiB  169 GiB  94.35  1.01  126      up\n18    ssd  2.91089   1.00000  2.9 TiB  2.7 TiB  2.7 TiB   474 KiB  5.7 GiB  198 GiB  93.35  1.00  126      up\n")),(0,r.kt)("p",null,"The first OSD (",(0,r.kt)("inlineCode",{parentName:"p"},"ID 1"),") has crossed the critical ",(0,r.kt)("strong",{parentName:"p"},"95% usage threshold"),", which\ncaused Ceph to enter a read-only state to prevent data loss or corruption."),(0,r.kt)("h3",{id:"2-inspecting-cluster-wide-usage"},"2. Inspecting Cluster-Wide Usage"),(0,r.kt)("p",null,"Next, we ran ",(0,r.kt)("inlineCode",{parentName:"p"},"ceph df")," to assess space usage across pools:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"[root@shiny-jacana ~]# ceph df\n--- RAW STORAGE ---\nCLASS     SIZE    AVAIL     USED  RAW USED  %RAW USED\nssd    140 TiB  8.7 TiB  131 TiB   131 TiB      93.74\n\n--- POOLS ---\nPOOL        ID   PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL\nvolumes      3  2048   65 TiB   17.09M  129 TiB  97.90    1.4 TiB\nimages       2   512  193 GiB   24.79k  385 GiB  11.92    1.4 TiB\nvms          4    64   60 GiB   15.46k  120 GiB   4.05    1.4 TiB\n")),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"volumes")," pool is using almost 98% of its quota, indicating that it\u2019s the\nmain driver of OSD saturation."),(0,r.kt)("h3",{id:"3-mitigating-the-ro-state"},"3. Mitigating the RO State"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"[root@shiny-jacana ~]# ceph osd set-full-ratio 0.97\nosd set-full-ratio 0.97\n")),(0,r.kt)("p",null,"This allowed the cluster to resume write operations. Keep in mind,\n",(0,r.kt)("strong",{parentName:"p"},"this is only a temporary solution"),". The underlying issue\n",(0,r.kt)("strong",{parentName:"p"},"lack of space")," still needs to be resolved."),(0,r.kt)("h2",{id:"how-to-prevent-this-in-the-future"},"How to Prevent This in the Future"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Set up proactive monitoring")," and use tools like\nDataDog to alert when OSD usage hits 80% or more.\n",(0,r.kt)("a",{parentName:"li",href:"https://openmetal.io/docs/manuals/openmetal-central/enable-datadog-monitoring"},"See this article"),"\nto enable Datadog Cloud Monitoring in ",(0,r.kt)("a",{parentName:"li",href:"https://openmetal.io/platform/openmetal-central-cloud-portal/"},"OpenMetal Central"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Automate snapshot cleanup"),"\nby implementing periodic audits for unused volumes and snapshots."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Plan capacity with headroom"),"\nand avoid running clusters above 80\u201385% usage for extended periods."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Scale out before you're at risk"),".\nIf usage trends consistently high, add more OSDs or expand storage.")),(0,r.kt)("admonition",{type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"Need to remove stuck images? Check out this\n",(0,r.kt)("a",{parentName:"p",href:"https://openmetal.io/docs/manuals/tutorials/ceph-dependencies"},"Ceph cleanup guide"),"\nto break parent-child dependencies and clear them out.")),(0,r.kt)("h2",{id:"in-summary"},"In Summary"),(0,r.kt)("p",null,"When Ceph enters a read-only state due to full OSDs, quick and informed action\nis essential. Temporarily adjusting the 'full-ratio' buys time, but it\u2019s not a\nlong-term fix. Clean up storage immediately and make sure you have monitoring\nand capacity planning in place to avoid recurrence. The earlier you respond to\nrising usage, the less disruptive the recovery will be."))}c.isMDXComponent=!0}}]);