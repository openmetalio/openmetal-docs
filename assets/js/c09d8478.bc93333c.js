"use strict";(self.webpackChunkopenmetal_docs=self.webpackChunkopenmetal_docs||[]).push([[7875],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>g});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),d=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},c=function(e){var t=d(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=d(a),h=r,g=u["".concat(l,".").concat(h)]||u[h]||p[h]||o;return a?n.createElement(g,i(i({ref:t},c),{},{components:a})):n.createElement(g,i({ref:t},c))}));function g(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:r,i[1]=s;for(var d=2;d<o;d++)i[d]=a[d];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},9784:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var n=a(7462),r=(a(7294),a(3905));const o={slug:"/engineers-notes/cloud101-cloud-storage",description:"Cloud 101: Cloud Storage Options"},i="Cloud 101: Cloud Storage Options",s={unversionedId:"engineers-notes/cloud101-cloud-storage",id:"engineers-notes/cloud101-cloud-storage",title:"Cloud 101: Cloud Storage Options",description:"Cloud 101: Cloud Storage Options",source:"@site/docs/engineers-notes/cloud101-cloud-storage.md",sourceDirName:"engineers-notes",slug:"/engineers-notes/cloud101-cloud-storage",permalink:"/docs/manuals/engineers-notes/cloud101-cloud-storage",draft:!1,editUrl:"https://github.com/openmetalio/openmetal-docs/blob/main/docs/engineers-notes/cloud101-cloud-storage.md",tags:[],version:"current",frontMatter:{slug:"/engineers-notes/cloud101-cloud-storage",description:"Cloud 101: Cloud Storage Options"},sidebar:"tutorialSidebar",previous:{title:"Building Windows Cloud Images on OpenMetal",permalink:"/docs/manuals/engineers-notes/building-windows-cloud-images-on-openmetal"},next:{title:"How to Configure Terraform to Automate OpenStack\u2019s Resources",permalink:"/docs/manuals/terraform/configure-terraform-to-automate-openstack-resources"}},l={},d=[{value:"Volume and Image Storage",id:"volume-and-image-storage",level:2},{value:"NVMe Drives and High IOPS Workloads",id:"nvme-drives-and-high-iops-workloads",level:3},{value:"Ceph Data Resiliency",id:"ceph-data-resiliency",level:2},{value:"Replication",id:"replication",level:3},{value:"Erasure Coding",id:"erasure-coding",level:3}],c={toc:d};function u(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"cloud-101-cloud-storage-options"},"Cloud 101: Cloud Storage Options"),(0,r.kt)("p",null,"As more and more businesses move their operations to the cloud, choosing the\nright storage option has become increasingly important. In this guide, we'll\ncover the basics of cloud storage, including the different types of storage\navailable, such as volume and image-based storage."),(0,r.kt)("p",null,"We'll also dive into data redundancy, which is essential for ensuring that your\ndata is always available and protected. We'll discuss replication, which\ninvolves making multiple copies of your data to guard against data loss. We'll\ncompare replication 2 versus 3, and explain the benefits of each."),(0,r.kt)("p",null,"Finally, we'll discuss erasure coding, which is an advanced technique for\nensuring data redundancy and protection. This guide is geared towards beginner\nand intermediate level engineers who want to understand the different cloud\nstorage options available and how to choose the right one for their organization"),(0,r.kt)("h2",{id:"volume-and-image-storage"},"Volume and Image Storage"),(0,r.kt)("p",null,"For those who are new to cloud storage, it's important to understand the\ndifference between volume and image storage. Volume storage is used for storing\ndata that needs to be accessed frequently, such as databases and application\ndata. Image storage, on the other hand, is used for storing virtual machine\nimages and other static data that doesn't change often. For most use-cases this\nwill hold true, but for workloads that need more performance, image-base storage\nmight be the best choice."),(0,r.kt)("h3",{id:"nvme-drives-and-high-iops-workloads"},"NVMe Drives and High IOPS Workloads"),(0,r.kt)("p",null,"When comparing block storage volumes and direct-attached image storage in a\ncloud computing environment, several factors should be taken into account, such\nas speed, reliability, live migrations, and more. Here's an overview of the\nbenefits and trade-offs of using block storage volumes versus direct-attached\nimage storage when using flash-based NVMe:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null}),(0,r.kt)("th",{parentName:"tr",align:null},"Block Storage Volumes"),(0,r.kt)("th",{parentName:"tr",align:null},"Direct-Attached Image Storage"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Speed")),(0,r.kt)("td",{parentName:"tr",align:null},"Can provide high performance, especially when using NVMe drives, which are optimized for low-latency and high-throughput access. However, they may experience some performance overhead due to network latency and the storage controller, compared to direct-attached image storage."),(0,r.kt)("td",{parentName:"tr",align:null},"Can offer better performance, as it eliminates the network latency and storage controller overhead associated with block storage volumes. NVMe drives, in particular, can provide ultra-low latency and high-speed access to data.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Reliability")),(0,r.kt)("td",{parentName:"tr",align:null},"Typically more reliable as they are managed by the cloud provider, offering built-in redundancy and data replication. This ensures data durability and availability even in the case of hardware failures."),(0,r.kt)("td",{parentName:"tr",align:null},"Reliability for direct-attached image storage depends on the hardware and implementation. Generally, it is less reliable than block storage volumes, as redundancy and data replication are not inherently provided, unless specifically configured.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Live Migrations")),(0,r.kt)("td",{parentName:"tr",align:null},"More flexible during live migrations, as they can be easily detached from one instance and attached to another. This makes it simpler to perform maintenance or scale resources without significant downtime."),(0,r.kt)("td",{parentName:"tr",align:null},"Can be more complex, as the data is tightly coupled with the instance. This can lead to longer downtimes and a more challenging migration process.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Scalability")),(0,r.kt)("td",{parentName:"tr",align:null},"Usually more scalable since they can be resized independently of the instances, allowing users to add or remove storage capacity on demand."),(0,r.kt)("td",{parentName:"tr",align:null},"Less scalable, as resizing the storage capacity typically requires resizing the instance itself or adding additional drives, which can be more time-consuming and disruptive.")))),(0,r.kt)("p",null,"Overall, block storage volumes are generally more reliable, flexible, and\nscalable, making them suitable for various cloud computing scenarios. On the\nother hand, direct-attached image storage can offer better performance and is a\ngood option for performance-sensitive workloads. The choice between the two\nshould be based on the specific requirements and priorities of your cloud environment."),(0,r.kt)("h2",{id:"ceph-data-resiliency"},"Ceph Data Resiliency"),(0,r.kt)("p",null,"Ceph provides data resiliency through replication and erasure coding.\nReplication involves making multiple copies of your data and storing them on\ndifferent nodes to protect against data loss. Ceph allows for replica 2 or\nreplica 3, providing varying levels of redundancy."),(0,r.kt)("p",null,"Erasure coding is an advanced technique that distributes data across multiple\nnodes and uses mathematical algorithms to create additional data chunks, which\nare stored on other nodes. This technique provides greater data protection with\nless storage overhead than traditional replication."),(0,r.kt)("h3",{id:"replication"},"Replication"),(0,r.kt)("p",null,"The primary factors to consider when choosing between a replica 2 and a replica\n3 configuration are performance, efficiency, and resiliency."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null}),(0,r.kt)("th",{parentName:"tr",align:null},"Replica 2"),(0,r.kt)("th",{parentName:"tr",align:null},"Replica 3"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Performance")),(0,r.kt)("td",{parentName:"tr",align:null},"In general, it will have better write performance than a replica 3, as data only needs to be written to two storage nodes, reducing the latency associated with data replication."),(0,r.kt)("td",{parentName:"tr",align:null},"May have slightly slower write performance than replica 2 due to the additional overhead of replicating data to a third storage node. Read performance, however, can be improved as the system can tolerate more simultaneous node failures without impacting data availability.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Efficiency")),(0,r.kt)("td",{parentName:"tr",align:null},"Uses less storage space compared to replica 3, as it only creates one additional copy of the data. This results in lower storage overhead, making it more cost-effective."),(0,r.kt)("td",{parentName:"tr",align:null},"While providing better data protection, it consumes more storage resources, as each piece of data is stored three times. This increases storage overhead and can be more costly.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Resiliency")),(0,r.kt)("td",{parentName:"tr",align:null},"While it provides redundancy, it is less resilient than replica 3. With only one additional copy of the data, the failure of two storage nodes simultaneously can result in data loss."),(0,r.kt)("td",{parentName:"tr",align:null},"Offers higher resiliency compared to replica 2. With two additional copies of the data, the system can tolerate the failure of up to two storage nodes without losing any data, making it more suitable for scenarios where data durability and availability are critical.")))),(0,r.kt)("p",null,"A replica 2 configuration might be more suitable for situations where storage efficiency\nand write performance are prioritized, while replica 3 is preferable for scenarios\nwhere data resiliency and higher fault tolerance are essential. The choice between\nreplica 2 and replica 3 should be carefully considered based on the specific needs\nand priorities of your storage environment."),(0,r.kt)("h3",{id:"erasure-coding"},"Erasure Coding"),(0,r.kt)("p",null,"Erasure coding is an advanced technique that Ceph uses to protect data with less\nstorage overhead than traditional replication. With erasure coding, data is divided\ninto multiple chunks, and then additional chunks are created using mathematical\nalgorithms. These additional chunks, called parity chunks, are distributed to other\nstorage nodes in the cluster."),(0,r.kt)("p",null,"When a data loss event occurs, the missing data can be reconstructed from the remaining\nchunks and parity chunks. The number of chunks that can be lost without data loss\ndepends on the chosen erasure code, which is a configurable parameter that determines\nthe level of redundancy and data protection desired."),(0,r.kt)("p",null,"In Ceph, erasure coding profiles are typically denoted as k+m, where k is the number\nof data chunks and m is the number of parity chunks. Some of the most common erasure\ncoding profiles used in Ceph are:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"EC 2+1")," - This profile generates one parity chunk for every two data chunks,\nresulting in a total of three chunks. This profile can tolerate the loss of a\nsingle chunk without data loss."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"EC 4+2")," - This profile generates two parity chunks for every four data chunks,\nresulting in a total of six chunks. This profile can tolerate the loss of up to\ntwo chunks without data loss."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"EC 8+3")," - This profile generates three parity chunks for every eight data chunks,\nresulting in a total of eleven chunks. This profile can tolerate the loss of up\nto three chunks without data loss."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"EC 16+4")," - This profile generates four parity chunks for every sixteen data\nchunks, resulting in a total of twenty chunks. This profile can tolerate the loss\nof up to four chunks without data loss.")),(0,r.kt)("p",null,"Erasure coding is ideal for large-scale deployments where storage efficiency is\ncritical, and the loss of a single storage node or drive is expected. However, erasure\ncoding requires additional processing power, so it may not be the best choice for\nsmaller, low-compute clusters."))}u.isMDXComponent=!0}}]);