"use strict";(self.webpackChunkopenmetal_docs=self.webpackChunkopenmetal_docs||[]).push([[4980],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>g});var i=t(67294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,i,r=function(e,n){if(null==e)return{};var t,i,r={},a=Object.keys(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=i.createContext({}),c=function(e){var n=i.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=c(e.components);return i.createElement(l.Provider,{value:n},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},d=i.forwardRef((function(e,n){var t=e.components,r=e.mdxType,a=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(t),d=r,g=u["".concat(l,".").concat(d)]||u[d]||m[d]||a;return t?i.createElement(g,o(o({ref:n},p),{},{components:t})):i.createElement(g,o({ref:n},p))}));function g(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var a=t.length,o=new Array(a);o[0]=d;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[u]="string"==typeof e?e:r,o[1]=s;for(var c=2;c<a;c++)o[c]=t[c];return i.createElement.apply(null,o)}return i.createElement.apply(null,t)}d.displayName="MDXCreateElement"},31735:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var i=t(87462),r=(t(67294),t(3905));const a={slug:"/private-ai/engineering-notes/monitoring-scheduling-vgpu",description:"Techniques for monitoring vGPU performance and scheduling GPU-backed workloads in OpenStack using traits and placement rules.",sidebar_position:6},o="Monitoring, Scheduling, and Performance Management of Virtual GPUs",s={unversionedId:"private-ai/engineering-notes/monitoring-scheduling-vgpu",id:"private-ai/engineering-notes/monitoring-scheduling-vgpu",title:"Monitoring, Scheduling, and Performance Management of Virtual GPUs",description:"Techniques for monitoring vGPU performance and scheduling GPU-backed workloads in OpenStack using traits and placement rules.",source:"@site/docs/private-ai/engineering-notes/monitoring-scheduling-vgpu.md",sourceDirName:"private-ai/engineering-notes",slug:"/private-ai/engineering-notes/monitoring-scheduling-vgpu",permalink:"/docs/manuals/private-ai/engineering-notes/monitoring-scheduling-vgpu",draft:!1,editUrl:"https://github.com/openmetalio/openmetal-docs/blob/main/docs/private-ai/engineering-notes/monitoring-scheduling-vgpu.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{slug:"/private-ai/engineering-notes/monitoring-scheduling-vgpu",description:"Techniques for monitoring vGPU performance and scheduling GPU-backed workloads in OpenStack using traits and placement rules.",sidebar_position:6},sidebar:"privateAiEngineeringNotesSidebar",previous:{title:"Installing and Managing NVIDIA GRID Drivers for Virtual GPU (vGPU) Deployments",permalink:"/docs/manuals/private-ai/engineering-notes/install-grid-driver-vgpu"},next:{title:"Managing MIG Devices and Automating Lifecycle Operations",permalink:"/docs/manuals/private-ai/engineering-notes/automate-mig-management"}},l={},c=[{value:"Monitoring vGPU Resources on the Host",id:"monitoring-vgpu-resources-on-the-host",level:2},{value:"Monitoring Inside Virtual Machines",id:"monitoring-inside-virtual-machines",level:2},{value:"Scheduling vGPUs in OpenStack",id:"scheduling-vgpus-in-openstack",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Concurrency and Isolation",id:"concurrency-and-isolation",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}],p={toc:c};function u(e){let{components:n,...t}=e;return(0,r.kt)("wrapper",(0,i.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"monitoring-scheduling-and-performance-management-of-virtual-gpus"},"Monitoring, Scheduling, and Performance Management of Virtual GPUs"),(0,r.kt)("p",null,"Effective use of virtual GPUs (vGPUs) in a private cloud requires visibility\ninto their performance and careful scheduling to ensure workloads are placed on\nthe appropriate resources. This article explains how to monitor vGPU usage,\noptimize resource scheduling in OpenStack, and understand performance factors\nwhen using MIG or SR-IOV with NVIDIA A100 GPUs."),(0,r.kt)("h2",{id:"monitoring-vgpu-resources-on-the-host"},"Monitoring vGPU Resources on the Host"),(0,r.kt)("p",null,"On the host system, administrators can use ",(0,r.kt)("inlineCode",{parentName:"p"},"nvidia-smi")," to view the status of MIG\ninstances or SR-IOV virtual functions:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"nvidia-smi\n")),(0,r.kt)("p",null,"The output includes:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Active GPU instances"),(0,r.kt)("li",{parentName:"ul"},"Compute instance IDs"),(0,r.kt)("li",{parentName:"ul"},"Memory usage"),(0,r.kt)("li",{parentName:"ul"},"Running processes")),(0,r.kt)("p",null,"For MIG configurations, detailed information on GPU and compute instance mapping\ncan be displayed with:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"nvidia-smi mig -lgi\nnvidia-smi mig -lci -gi <gpu_instance_id>\n")),(0,r.kt)("p",null,"For ongoing telemetry or dashboard integration, ",(0,r.kt)("strong",{parentName:"p"},"NVIDIA DCGM")," (Data Center\nGPU Manager) provides a metrics API that supports MIG devices. It can report:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"SM utilization"),(0,r.kt)("li",{parentName:"ul"},"Memory bandwidth"),(0,r.kt)("li",{parentName:"ul"},"ECC errors"),(0,r.kt)("li",{parentName:"ul"},"Application-level metrics")),(0,r.kt)("p",null,"DCGM supports Prometheus export and integrates with NVIDIA's container tools\nand Kubernetes plugins."),(0,r.kt)("h2",{id:"monitoring-inside-virtual-machines"},"Monitoring Inside Virtual Machines"),(0,r.kt)("p",null,"Once the NVIDIA GRID driver is installed in the VM, the ",(0,r.kt)("inlineCode",{parentName:"p"},"nvidia-smi")," utility\nbecomes available for end users. This allows developers to:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"View GPU memory consumption"),(0,r.kt)("li",{parentName:"ul"},"Monitor model inference jobs"),(0,r.kt)("li",{parentName:"ul"},"Identify bottlenecks in compute or memory usage")),(0,r.kt)("p",null,"Performance within the VM is isolated to the assigned MIG partition, ensuring\nconsistent and repeatable behavior even when multiple VMs share a physical GPU."),(0,r.kt)("h2",{id:"scheduling-vgpus-in-openstack"},"Scheduling vGPUs in OpenStack"),(0,r.kt)("p",null,"OpenStack Nova can treat MIG partitions or SR-IOV virtual functions as\ndiscrete resources. This is accomplished using ",(0,r.kt)("strong",{parentName:"p"},"resource classes")," and ",(0,r.kt)("strong",{parentName:"p"},"traits"),"."),(0,r.kt)("p",null,"Example: To create a flavor that requests a MIG device:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"openstack flavor create \\\n  --ram 8192 \\\n  --vcpus 4 \\\n  --disk 40 \\\n  --property resources:VGPU=1 \\\n  --property trait:CUSTOM_NVIDIA_1G5GB=required \\\n  gpu.mig.1g\n")),(0,r.kt)("p",null,"This allows the placement engine to match only compatible hosts that offer the\nspecific 1g.5gb MIG profile."),(0,r.kt)("p",null,"Resource traits can be extended using OpenStack placement APIs or custom drivers,\nallowing fine-grained control over workload scheduling based on available GPU profiles."),(0,r.kt)("h2",{id:"performance-considerations"},"Performance Considerations"),(0,r.kt)("p",null,"MIG partitions provide ",(0,r.kt)("strong",{parentName:"p"},"hardware-level isolation"),", meaning that each instance\nreceives a fixed amount of:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"GPU memory"),(0,r.kt)("li",{parentName:"ul"},"SM compute slices"),(0,r.kt)("li",{parentName:"ul"},"L2 cache and memory bandwidth")),(0,r.kt)("p",null,"This enables predictable inference throughput and latency. For example:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"A 1g.5gb profile offers 1/8 of the total GPU memory and 1/7 of the SMs"),(0,r.kt)("li",{parentName:"ul"},"A 3g.20gb profile provides more capacity for higher-throughput workloads")),(0,r.kt)("p",null,"Use cases such as:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Lightweight NLP inference"),(0,r.kt)("li",{parentName:"ul"},"Batch classification"),(0,r.kt)("li",{parentName:"ul"},"Prompt-based assistants")),(0,r.kt)("p",null,"...can run efficiently on smaller profiles, while high-resolution vision models\nor large LLMs may require 3g or 7g profiles."),(0,r.kt)("h2",{id:"concurrency-and-isolation"},"Concurrency and Isolation"),(0,r.kt)("p",null,"MIG allows up to ",(0,r.kt)("strong",{parentName:"p"},"7 concurrent GPU instances")," on a single A100. Each VM assigned\nto a MIG partition runs independently without sharing compute resources with neighbors."),(0,r.kt)("p",null,"This contrasts with ",(0,r.kt)("strong",{parentName:"p"},"time-slicing"),", where multiple processes share a single full\nGPU sequentially. MIG offers significantly improved predictability and latency\nfor real-time or production workloads."),(0,r.kt)("p",null,"When additional concurrency is needed within a single VM, ",(0,r.kt)("strong",{parentName:"p"},"CUDA MPS"),"\n(Multi-Process Service) can be used to parallelize inference tasks across multiple\nthreads or users within one MIG instance."),(0,r.kt)("h2",{id:"summary"},"Summary"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Use ",(0,r.kt)("inlineCode",{parentName:"p"},"nvidia-smi"),", DCGM, and OpenStack traits to monitor and schedule GPU workloads.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Match flavor specifications with the required MIG profiles for optimal placement.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"MIG delivers consistent performance and isolation, making it well-suited for\nparallel AI workloads in private clouds."))),(0,r.kt)("hr",null),(0,r.kt)("h2",{id:"next-steps"},"Next Steps"),(0,r.kt)("p",null,"The final article in this series will cover ",(0,r.kt)("strong",{parentName:"p"},"Best Practices for Managing MIG\nDevices and Automating Lifecycle Operations"),", including persistence strategies\nand automation tooling."))}u.isMDXComponent=!0}}]);