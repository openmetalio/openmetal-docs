"use strict";(self.webpackChunkopenmetal_docs=self.webpackChunkopenmetal_docs||[]).push([[630],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>m});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},u=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=c(a),h=i,m=p["".concat(l,".").concat(h)]||p[h]||d[h]||o;return a?n.createElement(m,r(r({ref:t},u),{},{components:a})):n.createElement(m,r({ref:t},u))}));function m(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:i,r[1]=s;for(var c=2;c<o;c++)r[c]=a[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},43353:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var n=a(87462),i=(a(67294),a(3905));const o={sidebar_position:3},r="How to Check Ceph's Status and Disk Usage",s={unversionedId:"operators-manual/day-2/check-ceph-status-disk-usage",id:"operators-manual/day-2/check-ceph-status-disk-usage",title:"How to Check Ceph's Status and Disk Usage",description:"Introduction",source:"@site/docs/operators-manual/day-2/check-ceph-status-disk-usage.md",sourceDirName:"operators-manual/day-2",slug:"/operators-manual/day-2/check-ceph-status-disk-usage",permalink:"/docs/manuals/operators-manual/day-2/check-ceph-status-disk-usage",draft:!1,editUrl:"https://github.com/openmetalio/openmetal-docs/blob/main/docs/operators-manual/day-2/check-ceph-status-disk-usage.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Introduction to Ceph",permalink:"/docs/manuals/operators-manual/day-2/introduction-to-ceph"},next:{title:"Maintaining OpenStack Software Updates",permalink:"/docs/manuals/operators-manual/day-2/maintenance"}},l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Check Ceph Status",id:"check-ceph-status",level:2},{value:"Check Ceph Disk Usage",id:"check-ceph-disk-usage",level:2},{value:"Check Ceph OSD individual Disk Usage",id:"check-ceph-osd-individual-disk-usage",level:2}],u={toc:c};function p(e){let{components:t,...a}=e;return(0,i.kt)("wrapper",(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"how-to-check-cephs-status-and-disk-usage"},"How to Check Ceph's Status and Disk Usage"),(0,i.kt)("h2",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"Ceph was selected as the storage solution for Private Cloud Core\nOpenStack clouds due to its ability store data in a replicated fashion.\nThe data stored in the Ceph cluster is accessible from any of your\ncloud's control plane nodes. The storage is considered shared across all\nnodes, which can make recovering an instance and its data trivial. As an\nadministrator of this cloud, we aim to provide you information about how\nyou can check the status of your Ceph cluster and see available disk\nusage using the command line."),(0,i.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Root access to your cloud's control plane nodes")),(0,i.kt)("h2",{id:"check-ceph-status"},"Check Ceph Status"),(0,i.kt)("p",null,"To check the status of your Ceph cluster, use ",(0,i.kt)("inlineCode",{parentName:"p"},"ceph status"),"."),(0,i.kt)("p",null,"For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# ceph status\n  cluster:\n    id:     34fa49b3-fff8-4702-8b17-4e8d873c845f\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum relaxed-flamingo,focused-capybara,lovely-ladybug (age 2w)\n    mgr: relaxed-flamingo(active, since 2w), standbys: focused-capybara, lovely-ladybug\n    osd: 4 osds: 4 up (since 3d), 4 in (since 3d)\n    rgw: 3 daemons active (focused-capybara.rgw0, lovely-ladybug.rgw0, relaxed-flamingo.rgw0)\n\n  task status:\n\n  data:\n    pools:   13 pools, 337 pgs\n    objects: 69.28k objects, 250 GiB\n    usage:   724 GiB used, 11 TiB / 12 TiB avail\n    pgs:     337 active+clean\n\n  io:\n    client:   121 KiB/s rd, 1.2 MiB/s wr, 137 op/s rd, 232 op/s wr\n")),(0,i.kt)("p",null,"The main things to watch are for the status of health and that all services are up.\nErrors will appear in this output as well to provide context for troubleshooting."),(0,i.kt)("h2",{id:"check-ceph-disk-usage"},"Check Ceph Disk Usage"),(0,i.kt)("p",null,"To check a cluster\u2019s data usage and data distribution among pools,\nuse the ",(0,i.kt)("inlineCode",{parentName:"p"},"df")," option. It is similar to the Linux ",(0,i.kt)("inlineCode",{parentName:"p"},"df")," command. You can run either\nthe ",(0,i.kt)("inlineCode",{parentName:"p"},"ceph df")," command or ",(0,i.kt)("inlineCode",{parentName:"p"},"ceph df detail")," command."),(0,i.kt)("p",null,"For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# ceph df\n--- RAW STORAGE ---\nCLASS  SIZE    AVAIL   USED     RAW USED  %RAW USED\nssd    12 TiB  11 TiB  720 GiB   724 GiB       6.08\nTOTAL  12 TiB  11 TiB  720 GiB   724 GiB       6.08\n\n--- POOLS ---\nPOOL                       ID  PGS  STORED   OBJECTS  USED     %USED  MAX AVAIL\ndevice_health_metrics       1    1  286 KiB        4  858 KiB      0    3.4 TiB\nimages                      2   32  7.6 GiB    1.02k   23 GiB   0.22    3.4 TiB\nvolumes                     3   32   88 GiB   23.61k  264 GiB   2.45    3.4 TiB\nvms                         4   32  144 GiB   39.92k  433 GiB   3.96    3.4 TiB\nbackups                     5   32      0 B        0      0 B      0    3.4 TiB\nmetrics                     6   32   25 MiB    4.49k  127 MiB      0    3.4 TiB\nmanila_data                 7   32      0 B        0      0 B      0    3.4 TiB\nmanila_metadata             8   32      0 B        0      0 B      0    3.4 TiB\n.rgw.root                   9   32  3.6 KiB        8   96 KiB      0    3.4 TiB\ndefault.rgw.log            10   32  3.4 KiB      207  384 KiB      0    3.4 TiB\ndefault.rgw.control        11   32      0 B        8      0 B      0    3.4 TiB\ndefault.rgw.meta           12    8    954 B        4   36 KiB      0    3.4 TiB\ndefault.rgw.buckets.index  13    8  2.2 MiB       11  6.6 MiB      0    3.4 TiB\n")),(0,i.kt)("p",null,"The RAW STORAGE section of the output provides an overview of the amount of storage\nthe storage cluster uses for data."),(0,i.kt)("p",null,"SIZE: The overall storage capacity managed by the storage cluster."),(0,i.kt)("p",null,"In the above example, if the SIZE is 90 GiB, it is the total size without the\nreplication factor, which is three by default. The total available capacity with\nthe replication factor is 90 GiB/3 = 30 GiB. Based on the full ratio, which is\n0.85% by default, the maximum available space is 30 GiB * 0.85 = 25.5 GiB"),(0,i.kt)("p",null,"AVAIL: The amount of free space available in the storage cluster."),(0,i.kt)("p",null,"In the above example, if the SIZE is 90 GiB and the USED space is 6 GiB, then\nthe AVAIL space is 84 GiB. The total available space with the replication factor,\nwhich is three by default, is 84 GiB/3 = 28 GiB"),(0,i.kt)("p",null,"USED: The amount of used space in the storage cluster consumed by user data,\ninternal overhead, or reserved capacity."),(0,i.kt)("h2",{id:"check-ceph-osd-individual-disk-usage"},"Check Ceph OSD individual Disk Usage"),(0,i.kt)("p",null,"To view OSD utilization statistics use, ",(0,i.kt)("inlineCode",{parentName:"p"},"ceph osd df")),(0,i.kt)("p",null,"For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# ceph osd df\nID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP    META     AVAIL    %USE  VAR   PGS  STATUS\n2    ssd  0.87329   1.00000  894 GiB   77 GiB   75 GiB  17 KiB  1.2 GiB  818 GiB  8.57  1.00  227      up\n0    ssd  0.87329   1.00000  894 GiB   77 GiB   75 GiB  17 KiB  1.2 GiB  818 GiB  8.57  1.00  227      up\n1    ssd  0.87329   1.00000  894 GiB   77 GiB   75 GiB  17 KiB  1.2 GiB  818 GiB  8.57  1.00  227      up\n                      TOTAL  2.6 TiB  230 GiB  226 GiB  52 KiB  3.6 GiB  2.4 TiB  8.57                   \nMIN/MAX VAR: 1.00/1.00  STDDEV: 0\n")))}p.isMDXComponent=!0}}]);