"use strict";(self.webpackChunkopenmetal_docs=self.webpackChunkopenmetal_docs||[]).push([[8082],{3905:(e,t,o)=>{o.d(t,{Zo:()=>u,kt:()=>f});var n=o(67294);function a(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function r(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,n)}return o}function i(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?r(Object(o),!0).forEach((function(t){a(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):r(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function l(e,t){if(null==e)return{};var o,n,a=function(e,t){if(null==e)return{};var o,n,a={},r=Object.keys(e);for(n=0;n<r.length;n++)o=r[n],t.indexOf(o)>=0||(a[o]=e[o]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)o=r[n],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(a[o]=e[o])}return a}var s=n.createContext({}),c=function(e){var t=n.useContext(s),o=t;return e&&(o="function"==typeof e?e(t):i(i({},t),e)),o},u=function(e){var t=c(e.components);return n.createElement(s.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var o=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),p=c(o),h=a,f=p["".concat(s,".").concat(h)]||p[h]||d[h]||r;return o?n.createElement(f,i(i({ref:t},u),{},{components:o})):n.createElement(f,i({ref:t},u))}));function f(e,t){var o=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=o.length,i=new Array(r);i[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:a,i[1]=l;for(var c=2;c<r;c++)i[c]=o[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,o)}h.displayName="MDXCreateElement"},36748:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>p,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var n=o(87462),a=(o(67294),o(3905));const r={sidebar_position:2},i="Introduction to Ceph",l={unversionedId:"day-2/introduction-to-ceph",id:"version-2.0/day-2/introduction-to-ceph",title:"Introduction to Ceph",description:"Introduction",source:"@site/operators_versioned_docs/version-2.0/day-2/introduction-to-ceph.md",sourceDirName:"day-2",slug:"/day-2/introduction-to-ceph",permalink:"/docs/manuals/operators-manual/operators_versioned_docs/version-2.0/day-2/introduction-to-ceph",draft:!1,editUrl:"https://github.com/openmetalio/openmetal-docs/blob/main/operators_versioned_docs/version-2.0/day-2/introduction-to-ceph.md",tags:[],version:"2.0",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"operatorsSidebar",previous:{title:"How Private Clouds are Deployed",permalink:"/docs/manuals/operators-manual/operators_versioned_docs/version-2.0/day-2/private-cloud-deployment-overview"},next:{title:"How to Check Ceph's Status and Disk Usage",permalink:"/docs/manuals/operators-manual/operators_versioned_docs/version-2.0/day-2/check-ceph-status-disk-usage"}},s={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Advantages of Ceph",id:"advantages-of-ceph",level:2},{value:"Data Resiliency",id:"data-resiliency",level:3},{value:"Ceph Scales Extremely Well",id:"ceph-scales-extremely-well",level:3},{value:"Disadvantages of Ceph",id:"disadvantages-of-ceph",level:2},{value:"Ceph Version Used by Private Clouds",id:"ceph-version-used-by-private-clouds",level:2},{value:"View Disk Usage of the Ceph Cluster",id:"view-disk-usage-of-the-ceph-cluster",level:2},{value:"Default Configuration for the Ceph Cluster",id:"default-configuration-for-the-ceph-cluster",level:2},{value:"Default Ceph Pools",id:"default-ceph-pools",level:3},{value:"Pool: images",id:"pool-images",level:4},{value:"Pool: volumes",id:"pool-volumes",level:4},{value:"Pool: vms",id:"pool-vms",level:4},{value:"Pool: backups",id:"pool-backups",level:4},{value:"Swift and Cinder Ceph Configuration",id:"swift-and-cinder-ceph-configuration",level:2},{value:"Reconfiguring your Ceph Cluster",id:"reconfiguring-your-ceph-cluster",level:2}],u={toc:c};function p(e){let{components:t,...o}=e;return(0,a.kt)("wrapper",(0,n.Z)({},u,o,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"introduction-to-ceph"},"Introduction to Ceph"),(0,a.kt)("h2",{id:"introduction"},"Introduction"),(0,a.kt)("p",null,"Ceph is an open-source, distributed storage system that provides object,\nblock and file storage interfaces from a single cluster. In this guide\nwe give a high-level overview of Ceph's services and how they are used\nin your OpenMetal Private Cloud."),(0,a.kt)("h2",{id:"advantages-of-ceph"},"Advantages of Ceph"),(0,a.kt)("p",null,"See ",(0,a.kt)("a",{parentName:"p",href:"https://ceph.io/en/discover/benefits/"},"Ceph's Benefits page")," for a\nmore complete understanding of the benefits Ceph provides."),(0,a.kt)("h3",{id:"data-resiliency"},"Data Resiliency"),(0,a.kt)("p",null,"With Ceph, your data storage is resilient either through the use of\nreplication or erasure coding. In our configuration, replication is used\nto create multiple copies of data. Data is replicated at the host level.\nA Private Cloud is deployed with three hosts. With our Ceph\nconfiguration, you could lose two hosts and still have all of your Ceph\ncluster's data."),(0,a.kt)("h3",{id:"ceph-scales-extremely-well"},"Ceph Scales Extremely Well"),(0,a.kt)("p",null,"Ceph is designed to scale horizontally into Petabyte figures."),(0,a.kt)("h2",{id:"disadvantages-of-ceph"},"Disadvantages of Ceph"),(0,a.kt)("p",null,"Data access times are not as quick compared to accessing data directly\nfrom a disk. Should your workload require very fast disk reads and\nwrites, consider using a compute only node, where instances can be spun\nup on local, LVM-backed storage."),(0,a.kt)("p",null,"For more, see ",(0,a.kt)("a",{parentName:"p",href:"https://openmetal.io/docs/manuals/tutorials/ephemeral_storage"},"Spin up an Instance with Ephemeral\nStorage"),"."),(0,a.kt)("h2",{id:"ceph-version-used-by-private-clouds"},"Ceph Version Used by Private Clouds"),(0,a.kt)("p",null,"With current Private Cloud deployments, Ceph's ",(0,a.kt)("a",{parentName:"p",href:"https://docs.ceph.com/en/latest/releases/octopus/"},"Octopus\nrelease")," is used."),(0,a.kt)("h2",{id:"view-disk-usage-of-the-ceph-cluster"},"View Disk Usage of the Ceph Cluster"),(0,a.kt)("p",null,"For more, see the guide on ",(0,a.kt)("a",{parentName:"p",href:"check-ceph-status-disk-usage"},"How to Check Ceph's Status and Disk Usage"),"."),(0,a.kt)("h2",{id:"default-configuration-for-the-ceph-cluster"},"Default Configuration for the Ceph Cluster"),(0,a.kt)("p",null,"Core Ceph services are deployed to each control plane node upon initial\ncloud deployment. This means each node comes installed with Ceph's\nMonitor, Manager, Object Gateway (RADOSGW), and Object Storage Daemon\n(OSD) services. Each node's secondary NVMe drive backs a Ceph OSD. Our\nCeph deployments are configured with host-level replication of data.\nYour cloud can lose all but one host and still retain all of the Ceph\ncluster's data."),(0,a.kt)("h3",{id:"default-ceph-pools"},"Default Ceph Pools"),(0,a.kt)("p",null,"By default, your Ceph cluster's data storage is logically divided into\npools, a concept associated with Ceph."),(0,a.kt)("p",null,"From a control plane node as root, we use ",(0,a.kt)("inlineCode",{parentName:"p"},"ceph osd lspools")," to list the\ndefault pools associated with a Private Cloud:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"1 device_health_metrics\n2 images\n3 volumes\n4 vms\n5 backups\n6 metrics\n7 manila_data\n8 manila_metadata\n9 .rgw.root\n10 default.rgw.log\n11 default.rgw.control\n12 default.rgw.meta\n13 default.rgw.buckets.index\n")),(0,a.kt)("p",null,"Next, we explain the purpose of some of the Ceph pools."),(0,a.kt)("h4",{id:"pool-images"},"Pool: images"),(0,a.kt)("p",null,"Operating System images maintained by Glance are stored in this pool"),(0,a.kt)("h4",{id:"pool-volumes"},"Pool: volumes"),(0,a.kt)("p",null,"Cinder stores any volumes created by your cloud in this pool."),(0,a.kt)("h4",{id:"pool-vms"},"Pool: vms"),(0,a.kt)("p",null,"When you spin up a volume-backed instance, Nova is configured to create\na volume for that instance in this pool."),(0,a.kt)("h4",{id:"pool-backups"},"Pool: backups"),(0,a.kt)("p",null,"Cinder stores volume backups within this pool."),(0,a.kt)("h2",{id:"swift-and-cinder-ceph-configuration"},"Swift and Cinder Ceph Configuration"),(0,a.kt)("p",null,"With Private Clouds, Swift and Cinder are the services configured to\nconnect to Ceph."),(0,a.kt)("p",null,"Swift, OpenStack's Object Storage service, connects directly to the Ceph\ncluster. With our setup you will not see configuration for Swift in a\nplace like ",(0,a.kt)("inlineCode",{parentName:"p"},"/etc/kolla/swift"),". This configuration is instead handled by\nCeph directly and can be viewed through ",(0,a.kt)("inlineCode",{parentName:"p"},"/etc/ceph/ceph.conf"),"."),(0,a.kt)("p",null,"Cinder, OpenStack's Block Storage service, is also configured to connect\nto Ceph. With Cinder, the are several services, of which ",(0,a.kt)("inlineCode",{parentName:"p"},"cinder-volume"),"\nand ",(0,a.kt)("inlineCode",{parentName:"p"},"cinder-backup")," are connected to Ceph. The Ceph configuration for\neach service can be viewed through\n",(0,a.kt)("inlineCode",{parentName:"p"},"/etc/kolla/cinder-volume/cinder.conf")," and\n",(0,a.kt)("inlineCode",{parentName:"p"},"/etc/kolla/cinder-backup/cinder.conf"),"."),(0,a.kt)("h2",{id:"reconfiguring-your-ceph-cluster"},"Reconfiguring your Ceph Cluster"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"WARNING","!")," -- Our current deployment system deploys a Private Cloud\nwith a known working state. Should you deviate from this state by\nadjusting your cloud's Ceph configuration you can no longer safely use\nthe functions in OpenMetal Central to add nodes to your cloud or add IP\nblocks. Should you use these functions, any custom configurations to\nCeph will be reverted. We are working on rolling out a new deployment\nsystem allowing custom cloud configurations. We can still add new nodes\nand IP blocks to your cloud but must do so manually. Please reach out to\nyour Account Manager should this apply to you."),(0,a.kt)("p",null,"Your Ceph cluster was deployed using Ceph Ansible. Any configuration\nchanges must be made using Ceph Ansible. For more information, see\n",(0,a.kt)("a",{parentName:"p",href:"../day-4/ceph-ansible/prepare-ceph-ansible"},"How to Prepare and Use Ceph Ansible")))}p.isMDXComponent=!0}}]);